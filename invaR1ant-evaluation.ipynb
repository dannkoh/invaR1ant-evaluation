{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2630e6e-cbc3-4033-aef4-8b17a9c25d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install openai\n",
    "!pip install z3-solver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d3a38-70ca-4638-9734-20da974bb8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a430e0",
   "metadata": {},
   "source": [
    "\n",
    "# invaR1ant-evaluation: SMT-LIB Constraint Generation via LLMs\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements an automated framework for leveraging large language models to generate SMT-LIB constraints that characterize worst-case execution paths in programs. The implementation analyzes how input conditions influence program performance at different input scales by systematically generating and testing constraints.\n",
    "\n",
    "## Background and Motivation\n",
    "\n",
    "This project addresses the challenge of characterizing worst-case execution paths in software systems. The approach uses examples of SMT-LIB constraints at smaller input sizes to prompt LLMs to generate corresponding constraints for larger input sizes.\n",
    "\n",
    "Unlike the CODE version (which generates Python code that can programmatically produce constraints), this notebook directly generates SMT-LIB constraint expressions within specialized tags:\n",
    "\n",
    "```\n",
    "<constants>(declare-const in0 Int)...</constants> <answer>(assert (and ...))...</answer>\n",
    "```\n",
    "\n",
    "This direct constraint generation approach may face challenges with token limitations when working with very large input sizes.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "The experiment conducts the following workflow:\n",
    "\n",
    "1. **Extract Constraints** from existing SMT files as examples (N=1 to N=18)\n",
    "2. **Query Language Models** (OpenAI API) to generate larger-scale constraints directly\n",
    "3. **Extract SMT-LIB Format Assertions** from responses (using regex patterns)\n",
    "4. **Validate Constraints** using Z3 theorem prover\n",
    "5. **Execute Test Cases** against a Java implementation (SortedListInsert)\n",
    "6. **Collect and Analyze Results** to verify execution path lengths\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **PromptBuilder**: Extracts constraints from SMT files and builds prompts for OpenAI\n",
    "- **OpenaiAPI**: Handles communication with OpenAI models with retry logic and rate limiting\n",
    "- **Services**: Provides utility functions for Z3 solver integration and test execution\n",
    "- **Experiment**: Orchestrates the end-to-end evaluation process\n",
    "\n",
    "## Technical Prerequisites\n",
    "\n",
    "- Python 3.12+ with packages:\n",
    "  - openai\n",
    "  - z3-solver\n",
    "  - tqdm\n",
    "  - dotenv (for environment variable management)\n",
    "- Java 8+ runtime (JDK 1.8.0_411 tested)\n",
    "- Z3 SMT solver installed and available in PATH\n",
    "- OpenAI API key set in environment variables as \"OPENAI_KEY\"\n",
    "\n",
    "## Experiment Configuration\n",
    "\n",
    "The experiment runs with configurable parameters:\n",
    "- Test input sizes (default: 250, 500, 1000)\n",
    "- OpenAI model (default: gpt-4o)\n",
    "- Maximum attempts per testing value (default: 10)\n",
    "- Temperature for model sampling (configurable)\n",
    "\n",
    "## Interpreting Results\n",
    "\n",
    "Results are saved as JSON files containing:\n",
    "- LLM-generated constraints\n",
    "- Z3 solver outputs\n",
    "- Test execution metrics\n",
    "- Execution path lengths (call counts)\n",
    "\n",
    "Each test case is considered successful when the actual call count matches the expected count (N+1), confirming the worst-case execution path was triggered.\n",
    "\n",
    "## Known Limitations\n",
    "\n",
    "- **Token Limitations**: For very large N values, the generated constraints may exceed model token limits\n",
    "- **Execution Time**: For large N values, Z3 solving and test execution may take considerable time\n",
    "- **Model Consistency**: Different OpenAI models may produce varying quality solutions\n",
    "- **Logging Duplication**: Multiple log handlers may cause duplicate output in Jupyter environments\n",
    "\n",
    "## What this notebook does **NOT** do\n",
    "\n",
    "This notebook does not cover the following aspects:\n",
    "- **Compiling Java Code**: The Java code has been compiled and is in the `/spf-wca/src/examples/` and `/spf-wca/build/examples/` directory.\n",
    "- **Running JPF/SPF on Java Code**: The Java code has been run using JPF and SPF-WCA. The results are in the `/spf-wca/sorted_list_insert/` directory.\n",
    "- **Compiling and building *JPF-CORE* *JPF-SYMBC* and *SPF-WCA***: The Java PathFinder (JPF) and SPF-WCA have been compiled and built. The JPF-CORE and JPF-SYMBC are in the `/jpf-core/` and `/jpf-symbc/` directories respectively. The SPF-WCA is in the `/spf-wca/` directory.\n",
    "\n",
    "---\n",
    "\n",
    "*Note: This notebook is part of research on program analysis and invariant generation using large language models. The approach demonstrates how LLMs can be used to generalize constraints for worst-case program behavior across different input scales.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f62783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from contextlib import suppress\n",
    "from pathlib import Path\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95314380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prompt:\n",
    "    promptStart1 = \"\"\"I'm experimenting with a program to understand how input conditions influence its worst-case performance, particularly in terms of finding the longest execution path as the input size increases. By conducting a worst-case analysis, I aim to identify constraints that define a valid input set at different input sizes (N). So far, I have found one possible set of correct constraints/conditions (not the only one) that characterize such valid inputs. Here they are:\\n\\n\"\"\"\n",
    "    promptStart2 = \"\"\"\\n\\nRespond with the complete constants inside <constants> </constants> tags and single valid SMT-LIB constraint expression inside <answer> </answer> tags. For example,<constants>(declare-const in0 Int)\\n(declare-const in1 Int)</constants> <answer>(assert (and (>= in0 0) (<= in1 10)))</answer>\"\"\"\n",
    "    promptStart3 = \"\"\"\\nWhat are the constraints for the input set of size N=\"\"\"\n",
    "    promptfeedback1 = \"\"\"Remember to include both <constants> </constants> tags and a single valid SMT-LIB constraint in <answer> </answer> tags. Your format was not correct.\"\"\"\n",
    "    promptfeedback2 = \"\"\"Remember to include both <constants> </constants> tags and a single valid SMT-LIB constraint in <answer> </answer> tags. Your constraint is not a valid SMTLIB expression.\"\"\"\n",
    "    promptsys = \"\"\"Give a complete SMT-LIB expression with constants in <constants> </constants> tags and a single valid constraint in <answer> </answer> tags. Do not shorten your answer.\"\"\"\n",
    "\n",
    "class PromptBuilder:\n",
    "    def __init__(self, constraints_dir=\"spf-wca/sorted_list_insert/verbose/heuristic\", file_prefix=\"SortedListInsert\") -> None:\n",
    "        constraints_dir = Path(os.getcwd()) / constraints_dir\n",
    "        self.constraints_dir = Path(constraints_dir)\n",
    "        self.file_prefix = file_prefix\n",
    "        self.constraints = self.__get_constraints()\n",
    "\n",
    "    def __get_constraints(self) -> str:\n",
    "        \"\"\"\n",
    "        Extract constraints from the SMT files in a sorted order.\n",
    "        \"\"\"\n",
    "        if not self.constraints_dir.exists():\n",
    "            raise FileNotFoundError(f\"Directory {self.constraints_dir} does not exist.\")\n",
    "\n",
    "        constraints = []\n",
    "        smt_files = list(self.constraints_dir.glob(f\"{self.file_prefix}_*.smt2\"))\n",
    "\n",
    "        if not smt_files:\n",
    "            raise FileNotFoundError(f\"No SMT files found in {self.constraints_dir}\")\n",
    "\n",
    "        def extract_n(file_path):\n",
    "            try:\n",
    "                n_str = str(file_path).split(self.file_prefix)[-1].split(\".smt2\")[0]\n",
    "                return int(n_str.strip(\"_\"))\n",
    "            except (IndexError, ValueError):\n",
    "                return float(\"inf\")  # Files that can't be parsed go at the end\n",
    "\n",
    "        smt_files.sort(key=extract_n)\n",
    "\n",
    "        for smt_file in smt_files:\n",
    "            n = extract_n(smt_file)\n",
    "            if n == float(\"inf\"):\n",
    "                continue\n",
    "\n",
    "            with open(smt_file, \"r\") as file:\n",
    "                smt_lines = [line.strip() for line in file if line.strip()]\n",
    "                assertions = [line for line in smt_lines if line.startswith(\"(assert\")]\n",
    "                assertions_str = \"\\n\".join(assertions)\n",
    "                constraints.append(f\"N={n}: {assertions_str}\")\n",
    "\n",
    "        return \"\\n\".join(constraints)\n",
    "\n",
    "    def build_prompt(self, testing_value: int) -> dict:\n",
    "        \"\"\"\n",
    "        Build the prompt for the OpenAI API.\n",
    "        \"\"\"\n",
    "        data = prompt()\n",
    "        return {\n",
    "            \"sys\": data.promptsys,\n",
    "            \"start\": data.promptStart1\n",
    "            + self.constraints\n",
    "            + data.promptStart2\n",
    "            + data.promptStart3\n",
    "            + str(testing_value)\n",
    "            + \"?\",\n",
    "            \"feedback-format\": data.promptfeedback1,\n",
    "            \"feedback-invalid\": data.promptfeedback2,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8aa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenaiAPI:\n",
    "    \"\"\"\n",
    "    OpenAI API interface.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str, temperature: int = None) -> None:\n",
    "        self.api_key = os.getenv(\"OPENAI_KEY\")\n",
    "        if not self.api_key:\n",
    "            msg = \"OPENAI_KEY environment variable not set\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.model = model\n",
    "        self.client = openai.OpenAI(api_key=self.api_key)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def chat(self, history: list, max_retries=5) -> str:\n",
    "        \"\"\"Chat with the OpenAI API with improved error handling.\"\"\"\n",
    "        retry_count = 0\n",
    "\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=history,\n",
    "                    temperature=self.temperature,\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except openai.RateLimitError as e:\n",
    "                retry_count += 1\n",
    "                self.logger.warning(f\"Rate limit error: {e}\")\n",
    "\n",
    "                if \"tokens\" in str(e) or \"must be reduced\" in str(e):\n",
    "                    self.logger.info(\"Token limit exceeded - reducing context\")\n",
    "                    history = self.__reduce_context(history)\n",
    "                    if not history:\n",
    "                        return \"Cannot reduce context further.\"\n",
    "                    continue\n",
    "\n",
    "                wait_time = min(60, 2**retry_count)\n",
    "                self.logger.info(f\"Rate limit exceeded. Waiting {wait_time}s\")\n",
    "                time.sleep(wait_time)\n",
    "            except openai.BadRequestError as e:\n",
    "                self.logger.error(f\"Bad request: {e}\")\n",
    "                if \"maximum context length\" in str(e):\n",
    "                    history = self.__reduce_context(history)\n",
    "                    if not history:\n",
    "                        return \"Error: Maximum context length exceeded.\"\n",
    "                    continue\n",
    "                return f\"Bad request: {str(e)}\"\n",
    "            except (openai.APIConnectionError, openai.APITimeoutError) as e:\n",
    "                retry_count += 1\n",
    "                wait_time = min(30, 2**retry_count)\n",
    "                self.logger.warning(f\"Connection error: {e}. Retrying in {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Unexpected error: {e}\")\n",
    "                return f\"Error: {str(e)}\"\n",
    "\n",
    "        return \"Maximum retry limit reached\"\n",
    "\n",
    "    def __reduce_context(self, history: list) -> list:\n",
    "        \"\"\"\n",
    "        Reduce the context by 1.\n",
    "        \"\"\"\n",
    "        if len(history) < 3:\n",
    "            return False\n",
    "\n",
    "        # Keep prompt and remove second item in the list\n",
    "        history.pop(2)\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac23cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Services:\n",
    "    \"\"\"\n",
    "    Services for Z3 solver and program execution.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def check_z3_installed(cls) -> bool:\n",
    "        \"\"\"\n",
    "        Check if Z3 solver is installed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"z3\", \"--version\"], capture_output=True, text=True)\n",
    "            return result.returncode == 0\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def Z3check(constants: str, constraints: str) -> dict:\n",
    "        \"\"\"\n",
    "        Check the satisfiability of the SMT2 file and return the test cases.\n",
    "\n",
    "        Args:\n",
    "            constants: Constants in the SMT2 file\n",
    "            constraints: Constraints in the SMT2 file\n",
    "\n",
    "        Returns:\n",
    "            None if unsat, dict of variable assignments if sat, or error message\n",
    "            Output: {'in': 1, 'in0': -1, 'in1': 0}\n",
    "\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.info(\"Invoking Z3 solver...\")\n",
    "        if not constants or not constraints:\n",
    "            logger.error(\"Missing constants or constraints\")\n",
    "            return {\"error\": \"Missing constants or constraints\"}\n",
    "        smt2 = Services.__format_smt2(constants=constants, constraints=constraints)\n",
    "        if not smt2:\n",
    "            print(\"Constraints not formatted correctly\")\n",
    "            return None\n",
    "        try:\n",
    "            proc = subprocess.run([\"z3\", \"-in\"], input=smt2, capture_output=True, text=True, timeout=60, check=False)\n",
    "\n",
    "            if proc.returncode != 0:\n",
    "                logger.error(f\"Z3 process error: {proc.stderr}\")\n",
    "                return {\"error\": f\"Z3 process error: {proc.stderr}\"}\n",
    "            output = proc.stdout.strip()\n",
    "            if \"unsat\" in output:\n",
    "                return None\n",
    "            elif \"sat\" in output:\n",
    "                return Services.__parse_z3_model(output, smt2)\n",
    "            else:\n",
    "                logger.error(f\"Unexpected Z3 output: {output}\")\n",
    "                return {\"error\": f\"Unexpected Z3 output: {output}\"}\n",
    "\n",
    "        except subprocess.TimeoutExpired:\n",
    "            logger.error(\"Z3 solver timed out after 60 seconds\")\n",
    "            return {\"error\": \"Z3 solver timed out after 60 seconds\"}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error running Z3: {e}\")\n",
    "            return {\"error\": f\"Error running Z3: {str(e)}\"}\n",
    "\n",
    "    @staticmethod\n",
    "    def __parse_z3_model(output: str, smt2: str) -> dict:\n",
    "        \"\"\"Parse Z3 model output into a dictionary.\"\"\"\n",
    "        try:\n",
    "            model_text = output.split(\"sat\", 1)[1].strip()\n",
    "            if not model_text:\n",
    "                return {}\n",
    "\n",
    "            model_dict = {}\n",
    "\n",
    "            # Find all define-fun blocks\n",
    "            import re\n",
    "\n",
    "            pattern = r\"\\(define-fun\\s+([^\\s]+)\\s+\\(\\)\\s+([^\\s]+)\\s+(.*?)\\)\"\n",
    "            matches = re.findall(pattern, model_text, re.DOTALL)\n",
    "\n",
    "            for var_name, var_type, var_value in matches:\n",
    "                value = var_value.strip()\n",
    "                if var_type == \"Int\":\n",
    "                    if value.startswith(\"(- \") and value.endswith(\")\"):\n",
    "                        value = -int(value[3:-1])\n",
    "                    else:\n",
    "                        with suppress(ValueError):\n",
    "                            value = int(value)\n",
    "                model_dict[var_name] = value\n",
    "\n",
    "            model_dict[\"smt2\"] = smt2\n",
    "            model_dict[\"z3_output\"] = output\n",
    "\n",
    "            return model_dict\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error parsing Z3 output: {str(e)}\", \"raw_output\": output}\n",
    "\n",
    "    @staticmethod\n",
    "    def __format_smt2(constants: str, constraints: str) -> str:\n",
    "        \"\"\"\n",
    "        Format the SMT2 file with the constants and constraints.\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "; Constants\n",
    "{constants}\n",
    "\n",
    "; Constraints\n",
    "{constraints}\n",
    "(check-sat)\n",
    "(get-model)\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def put_testcase(path: str, N: int, data: int) -> str:\n",
    "        \"\"\"\n",
    "        Put the testcase to the program.\n",
    "\n",
    "        Args:\n",
    "            path: Path to the program\n",
    "            N: Input size\n",
    "            data: Input data\n",
    "\n",
    "        Returns:\n",
    "            Output of the program\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return subprocess.check_output([\"java\", path, N, data], text=True, timeout=60)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            return f\"Error: {e!s}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea0326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\"\n",
    "    Experiment class that:\n",
    "     - Builds prompts for various input sizes.\n",
    "     - Queries the OpenAI API to get SMT-LIB constraints.\n",
    "     - Uses the Z3 solver to produce concrete test cases.\n",
    "     - Records the results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str, testing_values=None, max_attempts=10, temperature=None) -> None:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "        self.prompt_builder = PromptBuilder()\n",
    "        self.constraints = self.prompt_builder.constraints\n",
    "        self.max_attempts = max_attempts\n",
    "        self.testing_values = testing_values or [250, 500, 1000]\n",
    "\n",
    "        # Check Z3 installation before proceeding\n",
    "        if not Services.check_z3_installed():\n",
    "            raise RuntimeError(\"Z3 solver not found in PATH. Please install Z3.\")\n",
    "\n",
    "        try:\n",
    "            self.openai_api = OpenaiAPI(model,temperature=temperature)\n",
    "        except ValueError as e:\n",
    "            raise RuntimeError(f\"Failed to initialize OpenAI API: {e}\")\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "    def __extract_answer(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the SMT constraints from the OpenAI response.\n",
    "        \"\"\"\n",
    "        regex = r\"<constants>(.*?)</constants>.*?<answer>(.*?)</answer>\"\n",
    "        match = re.search(regex, text, re.DOTALL)\n",
    "        return (match.group(1), match.group(2)) if match else (None, None)\n",
    "\n",
    "    def run(self, save_results=True) -> dict:\n",
    "        \"\"\"Run the experiment for all testing values.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        for testing_value in self.testing_values:\n",
    "            self.logger.info(f\"Testing with N = {testing_value}\")\n",
    "            history = []\n",
    "            prompt_data = self.prompt_builder.build_prompt(testing_value)\n",
    "            history.append({\"role\": \"system\", \"content\": prompt_data[\"sys\"]})\n",
    "            history.append({\"role\": \"user\", \"content\": prompt_data[\"start\"]})\n",
    "            self.logger.info(f\"Prompt: {prompt_data['start']}\")\n",
    "\n",
    "            for attempt in range(1, self.max_attempts + 1):\n",
    "                self.logger.info(f\"Attempt {attempt} of {self.max_attempts}\")\n",
    "\n",
    "                # Get response from OpenAI\n",
    "                response = self.openai_api.chat(history)\n",
    "                self.logger.info(f\"OpenAI Response:\\n{response}\")\n",
    "\n",
    "                # Check if response has the expected format\n",
    "                if (\n",
    "                    \"<answer>\" in response\n",
    "                    and \"</answer>\" in response\n",
    "                    and \"<constants>\" in response\n",
    "                    and \"</constants>\" in response\n",
    "                ):\n",
    "                    constants, constraints = self.__extract_answer(response)\n",
    "\n",
    "                    if not constants or not constraints:\n",
    "                        self.logger.warning(\"No valid constraints found. Retrying...\")\n",
    "                        history.append({\"role\": \"assistant\", \"content\": response})\n",
    "                        history.append({\"role\": \"user\", \"content\": prompt_data[\"feedback-format\"]})\n",
    "                        continue\n",
    "\n",
    "                    # Get model from Z3\n",
    "                    model = Services.Z3check(constants, constraints)\n",
    "\n",
    "                    # Check for errors in the model\n",
    "                    if model is None:\n",
    "                        self.logger.warning(\"Constraints are unsatisfiable. Retrying...\")\n",
    "                        history.append({\"role\": \"assistant\", \"content\": response})\n",
    "                        history.append({\"role\": \"user\", \"content\": prompt_data[\"feedback-invalid\"]})\n",
    "                        continue\n",
    "\n",
    "                    if \"error\" in model:\n",
    "                        self.logger.warning(f\"Z3 error: {model['error']}. Retrying...\")\n",
    "                        history.append({\"role\": \"assistant\", \"content\": response})\n",
    "                        history.append({\"role\": \"user\", \"content\": prompt_data[\"feedback-invalid\"]})\n",
    "                        continue\n",
    "\n",
    "                    # Store initial result\n",
    "                    self.results[testing_value] = {\n",
    "                        \"Success\": False,\n",
    "                        \"Z3\": model,\n",
    "                        \"OpenAI\": response,\n",
    "                        \"Attempts\": attempt,\n",
    "                    }\n",
    "\n",
    "                    # Execute test only if we have input value\n",
    "                    if \"in\" in model:\n",
    "                        self.logger.info(f\"Executing the program with test case: {model['in']}\")\n",
    "                        EXPECTED_CALL_COUNT = testing_value + 1\n",
    "                        path = \"./test/SortedListInsert\"\n",
    "\n",
    "                        try:\n",
    "                            output = Services.put_testcase(path, testing_value, model[\"in\"])\n",
    "                            output_value = output.strip()\n",
    "\n",
    "                            try:\n",
    "                                call_count = int(output_value)\n",
    "                                success = call_count == EXPECTED_CALL_COUNT\n",
    "\n",
    "                                self.results[testing_value][\"Success\"] = success\n",
    "                                self.results[testing_value][\"Output\"] = output\n",
    "                                self.results[testing_value][\"ExpectedCallCount\"] = EXPECTED_CALL_COUNT\n",
    "                                self.results[testing_value][\"ActualCallCount\"] = call_count\n",
    "\n",
    "                                if success:\n",
    "                                    self.logger.info(f\"Success! Call count matches expected: {call_count}\")\n",
    "                                    break\n",
    "                                else:\n",
    "                                    self.logger.warning(\n",
    "                                        f\"Call count {call_count} doesn't match expected {EXPECTED_CALL_COUNT}\"\n",
    "                                    )\n",
    "\n",
    "                            except ValueError:\n",
    "                                self.logger.error(f\"Could not parse output as integer: {output_value}\")\n",
    "                                self.results[testing_value][\"Error\"] = f\"Failed to parse output: {output_value}\"\n",
    "\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"Error executing test: {e}\")\n",
    "                            self.results[testing_value][\"Error\"] = f\"Execution error: {str(e)}\"\n",
    "                    else:\n",
    "                        self.logger.warning(\"Model missing 'in' value. Cannot execute test.\")\n",
    "                        self.results[testing_value][\"Error\"] = \"Model missing 'in' value\"\n",
    "\n",
    "                    # Break if we have a valid model, even if execution failed\n",
    "                    break\n",
    "                else:\n",
    "                    self.logger.warning(\"Response did not contain valid tags. Retrying...\")\n",
    "                    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "                    history.append({\"role\": \"user\", \"content\": prompt_data[\"feedback-format\"]})\n",
    "            else:\n",
    "                self.logger.error(f\"Failed after {self.max_attempts} attempts for N={testing_value}\")\n",
    "                self.results[testing_value] = {\"Success\": False, \"Error\": f\"Failed after {self.max_attempts} attempts\"}\n",
    "\n",
    "        # Save results if requested\n",
    "        if save_results:\n",
    "            self.save_results()\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        self.logger.info(f\"Experiment completed in {duration:.2f} seconds\")\n",
    "\n",
    "    def save_results(self, filename=None):\n",
    "        \"\"\"Save results to a JSON file.\"\"\"\n",
    "        if not filename:\n",
    "            timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            filename = f\"experiment_results_{timestamp}.json\"\n",
    "\n",
    "        try:\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(self.results, f, indent=2)\n",
    "            self.logger.info(f\"Results saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save results: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\"gpt-4o\")\n",
    "experiment.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
